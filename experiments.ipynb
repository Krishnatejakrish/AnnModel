import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import pickle

# Load the dataset
data = pd.read_csv('./Churn_Modelling.csv')

# Drop irrelevant columns (they don't help in prediction)
data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Convert 'Gender' (Male/Female) to numbers (1/0) using LabelEncoder
label_encoder_gender = LabelEncoder()
data['Gender'] = label_encoder_gender.fit_transform(data['Gender'])

# One-hot encode the 'Geography' column (France/Germany/Spain ‚Üí separate columns)
one_hot_encoder_geo = OneHotEncoder()
geo_encoder = one_hot_encoder_geo.fit_transform(data[['Geography']])

# Get new column names like 'Geography_France', etc.
geo_encoded_df = pd.DataFrame(
    geo_encoder.toarray(), 
    columns=one_hot_encoder_geo.get_feature_names_out(['Geography'])
)

# Combine the one-hot encoded 'Geography' columns with the main data
# First, drop the original 'Geography' column
data = pd.concat([data.drop('Geography', axis=1), geo_encoded_df], axis=1)

# Preview the final preprocessed dataset
print(data.head())

print(geo_encoded_df)

# | Column      | Type                   | What to do?           |
# | ----------- | ---------------------- | --------------------- |
# | `Gender`    | Categorical (2 values) | ‚úÖ Use `LabelEncoder`  |
# | `Geography` | Categorical (3 values) | ‚úÖ Use `OneHotEncoder` |
# | `Age`       | Numerical              | ‚ö†Ô∏è Optionally scale   |
# | `Balance`   | Numerical              | ‚ö†Ô∏è Optionally scale   |
# | `Exited`    | Target (0/1)           | ‚úÖ Leave as-is         |


# combine one hot encoded columns with the original data
with open('label_encoder_gender.pkl','wb') as file:
    pickle.dump(label_encoder_gender,file)
    
with open('one_hot_encoder_geo.pkl','wb') as file:
    pickle.dump(one_hot_encoder_geo,file)
    
#  divide the dataset into independent and dependent features
X = data.drop('Exited', axis=1)
y = data['Exited']

## split the data in training and testing set

# Split the data: 80% for training, 20% for testing (random_state = 42 makes the split repeatable)
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features: center around 0 with same scale
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # Fit only on training data
X_test = scaler.transform(X_test) 

print('xtrain',X_train)# Apply same transformation to test data
print('xtest',X_test)

with open('scaler.pkl','wb') as file:
    pickle.dump(scaler,file)
    

# ==============================================
# üí° Neural Network Layer Overview (Cheat Sheet)
# ==============================================
# | Layer         | Purpose                                           | Example                          |
# |---------------|---------------------------------------------------|----------------------------------|
# | Input Layer   | Takes raw features (like Age, Balance, Gender)    | input_shape = (num_features,)    |
# | Hidden Layer  | Learns hidden patterns in data (the "brain")      | Dense(16, activation='relu')     |
# |               | Applies activation to extract relationships       | Dense(64), Dense(32), etc.       |
# | Output Layer  | Produces final result (prediction)                | Dense(1, activation='sigmoid')   |
# ------------------------------------------------------------------------------
# ‚úÖ Notes:
# - Input layer neurons = number of input features (e.g., 10 columns ‚Üí input_shape=(10,))
# - Hidden layers can have any number of neurons (common: 8, 16, 32, 64, 128...)
# - You can stack multiple hidden layers to make the network deeper
# - Output layer depends on task:
#     ‚Ä¢ Binary classification ‚Üí 1 neuron + sigmoid
#     ‚Ä¢ Multi-class classification ‚Üí N neurons + softmax
#     ‚Ä¢ Regression ‚Üí 1 neuron (no activation or use 'linear')
# ==============================================
print(X_train.shape[1])

import tensorflow as tf
    # from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
import datetime
  
    
    # ================================================
# üí° TensorFlow & Keras Imports ‚Äî Simple Breakdown
# ================================================
# | Import                                               |What it Does                                            |
# |--------------------------------------------|--------------------------------------------------------          |
# | import tensorflow as tf                              | Main deep learning framework (Google‚Äôs library)        |
# | from tensorflow.keras.models import Sequential       | Lets you build models layer-by-layer (Sequential type) |
# | from tensorflow.keras.layers import Dense            | Fully connected layer for learning patterns            |
# | from tensorflow.keras.callbacks import EarlyStopping | Stops training early if model stops improving          |
# | from tensorflow.keras.callbacks import tensorboard   | Tool to visualize training (loss/accuracy)             |
# ================================================

      
    ## build our ann model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with 64 neurons
    Dense(32, activation='relu'),  # Hidden layer with 32 neurons
    Dense(1, activation='sigmoid')  # Output layer with 1 neuron
    ])

# [Input Features: 10 columns]
#         ‚Üì
# Dense(64) ‚Äî ReLU Activation
#         ‚Üì
# Dense(32) ‚Äî ReLU Activation
#         ‚Üì
# Dense(1)  ‚Äî Sigmoid ‚Üí Output: 0 or 1

print(model.summary())  # Print model summary to see layer details

# optimizers
# import tensorflow
# opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)  # Adam optimizer with learning rate 0.001
# loss = tensorflow.keras.Binary_crossentropy()  # Binary crossentropy loss function
# print(opt)                        |
                                    #| we can use either that opt n d loss variable or else we can use direclty in the model 
                                    # #compile the model
model.compile(optimizer = "adam",loss = "binary_crossentropy", metrics = ["accuracy"])

# | Term           | What it means                         |
# | -------------- | ------------------------------------- |
# | Optimizer      | Decides how weights are updated       |
# | Adam           | Adaptive optimizer (fast + smart)     |
# | learning\_rate | Controls the size of each update step |

# | Term     | Meaning                                                                   |
# | -------- | ------------------------------------------------------------------------- |
# | Accuracy | Percentage of correct predictions (out of total predictions made)         |
# | Example  | If the model predicts correctly on 850 out of 1000 samples ‚Üí 85% accuracy |
# | Loss     | Measures how far off predictions are from actual values (lower is better) |
# | Example  | If the model predicts 0.2 when it should be 1 ‚Üí high loss (bad prediction) |
# | Binary Crossentropy | Loss function for binary classification (0/1) tasks |
# | Example  | If the model predicts 0.8 when it should be 1 ‚Üí binary crossentropy loss |

# set up the tensor board
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

# Set up Early stopping to prevent overfitting

early_stopping = EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    patience=10,          # Stop after 5 epochs without improvement
    restore_best_weights=True  # Restore weights from the best epoch    
)
# Weights: Model's internal values that are learned to make better predictions
# Epochs: Number of times the model sees the entire training dataset

# Train the model
model_train = model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), callbacks=[tensorboard_callback, early_stopping])
print(model_train)
model.save('model.h5')

## load tensor board Extension
# %load_ext tensorboard

# Start TensorBoard to visualize training
# %tensorboard --logdir logs/fit


import tensorflow as tf
from tensorflow.keras.models import load_model
import pickle
import pandas as pd
import numpy as np

### load the rrained model, scaler pickle,one hot
model = load_model('model.h5')
## load the one_hot_encoder and scaler
with open('one_hot_encoder_geo.pkl', 'rb') as file:
    one_hot_encoder_geo = pickle.load(file)

## load the label_encoder
with open ('label_encoder_gender.pkl', 'rb') as file:
    label_encoder_gender = pickle.load(file)

with open('scaler.pkl', 'rb') as file:
    scaler = pickle.load(file)

# Pickle stores the trained object‚Äôs memory (like mean/std for scaler)
# It can be loaded later to apply the same transformation without retraining
# example input data
input_data = {
    'CreditScore': 600,
    'Geography': 'France',
    'Gender': 'Male',
    'Age': 40,
    'Tenure': 3,
    'Balance': 60000,
    'NumOfProducts':2,
    'HasCrCard': 1,
    'IsActiveMember': 1,
    'EstimatedSalary': 50000
}

geo_encoder = one_hot_encoder_geo.transform([[input_data['Geography']]]).toarray()
geo_encoded_df = pd.DataFrame(
    geo_encoder,
    columns=one_hot_encoder_geo.get_feature_names_out(['Geography'])
)
geo_encoded_df

input_df = pd.DataFrame([input_data])
input_df

## encode  categorical varibales
input_df['Gender'] = label_encoder_gender.transform(input_df['Gender'])
input_df

## concatination one hot encoded
input_df = pd.concat([input_df.drop('Geography', axis=1), geo_encoded_df], axis=1)
input_df


# scaling the input data
input_scaled = scaler.transform(input_df)
input_scaled

#predict churn
prediction = model.predict(input_scaled)
prediction

prediction_proba = prediction[0][0]
prediction_proba

if prediction_proba > 0.5:
    print("Customer is likely to leave")
else:
    print("Customer is likely to stay")
    

